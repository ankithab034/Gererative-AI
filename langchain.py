-*- medical assistant: utf-8 -*-

"""genai3langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gbi9OgzC5F9oRaMR8nKjSWIl80rasGGx
"""

!pip install -q langchain-community huggingface_hub transformers sentence_transformers faiss-cpu pandas matplotlib

!pip install -U langchain langchain-huggingface transformers
from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable

# Setup chat model (BlenderBot)
chat_pipeline = pipeline("text2text-generation", model="facebook/blenderbot-400M-distill", device=-1)

# Wrap in LangChain
llm = HuggingFacePipeline(pipeline=chat_pipeline)

# Create memory
memory = ConversationBufferMemory(return_messages=True)

# Chat prompt with memory
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful medical assistant. Provide helpful advice without diagnosing directly."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# Runnable chain (LangChain 1.0+ style)
chain = prompt | llm

print("ðŸ¤– Medical Assistant is ready. Type 'exit' to quit.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break

    # History from memory
    history = memory.chat_memory.messages

    # Get response
    response = chain.invoke({"input": user_input, "history": history})

    print("Assistant:", response)

    # Update memory
    memory.chat_memory.add_user_message(user_input)
    memory.chat_memory.add_ai_message(response)
